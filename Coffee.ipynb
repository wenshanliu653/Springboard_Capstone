{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To scrape the coffee data from the website_v2\n",
    "- Data Collection\n",
    "- Data Organization\n",
    "- Data Definition\n",
    "- Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Data Collection\n",
    "#  To scrape the coffee data from the website\n",
    "# first : Need to get the 'Coffe Name'\n",
    "# second : Use Coffee Name to get the Coffee Review Detail\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'coffee_name': 'Kenya AA Aries Return Espresso', 'coffee_review_url': 'https://www.coffeereview.com/review/kenya-aa-aries-return-espresso/', 'coffee_brand_url': 'https://www.facebook.com/4ArtsZeroDefectCoffees'}, {'coffee_name': 'Kenya All’s Well Peaberry Espresso', 'coffee_review_url': 'https://www.coffeereview.com/review/kenya-alls-well-peaberry-espresso/', 'coffee_brand_url': 'https://www.facebook.com/4ArtsZeroDefectCoffees'}, {'coffee_name': 'Espresso No. 6 Finca San José Ocaña', 'coffee_review_url': 'https://www.coffeereview.com/review/espresso-no-6-finca-san-jose-ocana/', 'coffee_brand_url': 'https://elgran.cafe/'}, {'coffee_name': 'Guatemala Finca Santa Isabel SL28 COE #13', 'coffee_review_url': 'https://www.coffeereview.com/review/guatemala-finca-santa-isabel-sl28-coe-13/', 'coffee_brand_url': 'https://elgran.cafe/'}, {'coffee_name': 'Kahiko Orange', 'coffee_review_url': 'https://www.coffeereview.com/review/kahiko-orange/', 'coffee_brand_url': 'http://bit.ly/2oBDGmP'}, {'coffee_name': 'Kenya Washed Yara Estate PB TOP', 'coffee_review_url': 'https://www.coffeereview.com/review/kenya-washed-yara-estate-pb-top/', 'coffee_brand_url': 'http://www.kakalovecafe.com.tw/'}, {'coffee_name': 'Kona Orange', 'coffee_review_url': 'https://www.coffeereview.com/review/kona-orange/', 'coffee_brand_url': 'http://bit.ly/2oBDGmP'}, {'coffee_name': 'Colombia Wilton Benitez Sudan Rume', 'coffee_review_url': 'https://www.coffeereview.com/review/colombia-wilton-benitez-sudan-rume/', 'coffee_brand_url': 'https://utopiancoffee.com/'}, {'coffee_name': 'Ethiopia Washed Gute 74110 Lot JH', 'coffee_review_url': 'https://www.coffeereview.com/review/ethiopia-washed-gute-74110-lot-jh/', 'coffee_brand_url': 'http://www.kakalovecafe.com.tw/'}, {'coffee_name': 'Ethiopia Natural Sidama Bura Karamo 74158', 'coffee_review_url': 'https://www.coffeereview.com/review/ethiopia-natural-sidama-bura-karamo-74158/', 'coffee_brand_url': 'http://www.kakalovecafe.com.tw/'}, {'coffee_name': 'Taiwan Honey Alishan Ming Yang Yuan Gesha', 'coffee_review_url': 'https://www.coffeereview.com/review/taiwan-honey-alishan-ming-yang-yuan-gesha/', 'coffee_brand_url': 'http://www.kakalovecafe.com.tw/'}, {'coffee_name': 'Kenya Washed Kiambu AB TOP Lot Yerihar', 'coffee_review_url': 'https://www.coffeereview.com/review/kenya-washed-kiambu-ab-top-lot-yerihar/', 'coffee_brand_url': 'http://www.kakalovecafe.com.tw/'}, {'coffee_name': 'Kenya Ibonia Estate', 'coffee_review_url': 'https://www.coffeereview.com/review/kenya-ibonia-estate/', 'coffee_brand_url': 'https://roadmapcoffeeworks.com/'}, {'coffee_name': 'Ethiopia Worka Sakaro', 'coffee_review_url': 'https://www.coffeereview.com/review/ethiopia-worka-sakaro-2/', 'coffee_brand_url': 'https://roadmapcoffeeworks.com/'}, {'coffee_name': 'Guatemala Washed Huehuetenango Joya Verde', 'coffee_review_url': 'https://www.coffeereview.com/review/guatemala-washed-huehuetenango-joya-verde/', 'coffee_brand_url': 'http://www.kakalovecafe.com.tw/'}, {'coffee_name': 'Ethiopia Washed Guji Uraga Tebe Burka', 'coffee_review_url': 'https://www.coffeereview.com/review/ethiopia-washed-guji-uraga-tebe-burka/', 'coffee_brand_url': 'http://www.kakalovecafe.com.tw/'}, {'coffee_name': 'Colombia Decaf Granja Paraiso 92', 'coffee_review_url': 'https://www.coffeereview.com/review/colombia-decaf-granja-paraiso-92/', 'coffee_brand_url': 'http://www.kakalovecafe.com.tw/'}, {'coffee_name': 'Ethiopia Natural Sidama ALO Village SKY Project FL', 'coffee_review_url': 'https://www.coffeereview.com/review/ethiopia-natural-sidama-alo-village-sky-project-fl/', 'coffee_brand_url': 'http://www.kakalovecafe.com.tw/'}, {'coffee_name': 'Taiwan Natural Alishan Chi Tsai Liu Li Ecological Farm', 'coffee_review_url': 'https://www.coffeereview.com/review/taiwan-natural-alishan-chi-tsai-liu-li-ecological-farm/', 'coffee_brand_url': 'http://www.kakalovecafe.com.tw/'}, {'coffee_name': 'Wilton Benitez SL28', 'coffee_review_url': 'https://www.coffeereview.com/review/wilton-benitez-sl28/', 'coffee_brand_url': 'https://www.jbccoffeeroasters.com/product-category/coffee/'}, {'coffee_name': 'Kolla Bolcha #6 Ethiopia', 'coffee_review_url': 'https://www.coffeereview.com/review/kolla-bolcha-6-ethiopia/', 'coffee_brand_url': 'https://www.jbccoffeeroasters.com/product-category/coffee/'}, {'coffee_name': 'Bener Meriah Sumatra', 'coffee_review_url': 'https://www.coffeereview.com/review/bener-meriah-sumatra/', 'coffee_brand_url': 'https://www.jbccoffeeroasters.com/product-category/coffee/'}, {'coffee_name': 'Ethiopia Yirgacheffe Hama Honey', 'coffee_review_url': 'https://www.coffeereview.com/review/ethiopia-yirgacheffe-hama-honey/', 'coffee_brand_url': 'https://shop.cozyhousecoffee.com/'}, {'coffee_name': 'Colombia Finca Santuario Red Bourbon Natural', 'coffee_review_url': 'https://www.coffeereview.com/review/colombia-finca-santuario-red-bourbon-natural/', 'coffee_brand_url': 'https://shop.cozyhousecoffee.com/'}, {'coffee_name': 'Ethiopia Yirgacheffe Gutiti Gargari Natural', 'coffee_review_url': 'https://www.coffeereview.com/review/ethiopia-yirgacheffe-gutiti-gargari-natural/', 'coffee_brand_url': 'https://shop.cozyhousecoffee.com/'}, {'coffee_name': '130 Blend', 'coffee_review_url': 'https://www.coffeereview.com/review/130-blend/', 'coffee_brand_url': 'https://jauntcoffee.com/'}, {'coffee_name': 'Fruits Melody', 'coffee_review_url': 'https://www.coffeereview.com/review/fruits-melody/', 'coffee_brand_url': 'https://www.coffeeland.com.tw/'}, {'coffee_name': 'Matriarch', 'coffee_review_url': 'https://www.coffeereview.com/review/matriarch/', 'coffee_brand_url': 'https://jauntcoffee.com/'}, {'coffee_name': 'Murphinator', 'coffee_review_url': 'https://www.coffeereview.com/review/murphinator/', 'coffee_brand_url': 'https://jauntcoffee.com/'}, {'coffee_name': 'Taiwan Symphony', 'coffee_review_url': 'https://www.coffeereview.com/review/taiwan-symphony/', 'coffee_brand_url': 'https://www.coffeeland.com.tw/'}, {'coffee_name': 'Blue Notes', 'coffee_review_url': 'https://www.coffeereview.com/review/blue-notes/', 'coffee_brand_url': 'https://www.coffeeland.com.tw/'}, {'coffee_name': 'Ethiopia Dale', 'coffee_review_url': 'https://www.coffeereview.com/review/ethiopia-wale/', 'coffee_brand_url': 'https://speedwellcoffee.com/'}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "df = []\n",
    "try:\n",
    "    #for page in range(1, 140): # total page = 139\n",
    "    for page in range(1, 3): #test\n",
    "        \n",
    "        url = f'https://www.coffeereview.com/advanced-search/page/{page}/?keyword=&search=Search+Now&locations=all&score_all=on&score_96_100=on&score_93_95=on&score_90_92=on&score_85_89=on&score_85=on#results'\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}  \n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        #print(soup)\n",
    "        #print(soup.prettify()) \n",
    "        \n",
    "        reviews = soup.select('.review-template')  # First, find 'the container' of each review\n",
    "        \n",
    "        for review in reviews:\n",
    "            coffee_name = review.select_one('.entry-content h2 a')\n",
    "            coffee_brand = review.select_one('.row-3 .col-2 a')\n",
    "            \n",
    "            if coffee_name and coffee_brand:\n",
    "                time.sleep(random.uniform(2, 5))\n",
    "                \n",
    "                df.append({\n",
    "                    'coffee_name': coffee_name.text,\n",
    "                    'coffee_review_url': coffee_name['href'],\n",
    "                    'coffee_brand_url': coffee_brand['href']\n",
    "                })\n",
    "                \n",
    "        time.sleep(3)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "print(df)\n",
    "df_to_csv = pd.DataFrame(df)\n",
    "df_to_csv.to_csv('coffee_brand.csv', index=False) # save the data to csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 coffee_name  \\\n",
      "0             Kenya AA Aries Return Espresso   \n",
      "1         Kenya All’s Well Peaberry Espresso   \n",
      "2        Espresso No. 6 Finca San José Ocaña   \n",
      "3  Guatemala Finca Santa Isabel SL28 COE #13   \n",
      "4                              Kahiko Orange   \n",
      "\n",
      "                                   coffee_review_url  \\\n",
      "0  https://www.coffeereview.com/review/kenya-aa-a...   \n",
      "1  https://www.coffeereview.com/review/kenya-alls...   \n",
      "2  https://www.coffeereview.com/review/espresso-n...   \n",
      "3  https://www.coffeereview.com/review/guatemala-...   \n",
      "4  https://www.coffeereview.com/review/kahiko-ora...   \n",
      "\n",
      "                                  coffee_brand_url  \\\n",
      "0  https://www.facebook.com/4ArtsZeroDefectCoffees   \n",
      "1  https://www.facebook.com/4ArtsZeroDefectCoffees   \n",
      "2                             https://elgran.cafe/   \n",
      "3                             https://elgran.cafe/   \n",
      "4                            http://bit.ly/2oBDGmP   \n",
      "\n",
      "                            coffee_web_name  \n",
      "0            kenya-aa-aries-return-espresso  \n",
      "1         kenya-alls-well-peaberry-espresso  \n",
      "2        espresso-no-6-finca-san-jose-ocana  \n",
      "3  guatemala-finca-santa-isabel-sl28-coe-13  \n",
      "4                             kahiko-orange  \n"
     ]
    }
   ],
   "source": [
    "# read the coffee brand data\n",
    "df_coffee_brand = pd.read_csv('coffee_brand.csv')\n",
    "\n",
    "# extracte coffee_web_name to new column 'coffee_web_name'  from 'coffee_review_url'\n",
    "df_coffee_brand['coffee_web_name'] = df_coffee_brand['coffee_review_url'].apply(lambda x: x.split('review/')[-1].rstrip('/'))\n",
    "\n",
    "\n",
    "print(df_coffee_brand.head())\n",
    "\n",
    "# save the data to csv\n",
    "df_coffee_brand.to_csv('coffee_brand.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roaster: Simon Hsieh Aroma Roast Coffees\n",
      "Title: Kenya AA Aries Return Espresso\n",
      "Score: 96\n",
      "----------------------------------------\n",
      "Roaster: Simon Hsieh Aroma Roast Coffees\n",
      "Title: Kenya All’s Well Peaberry Espresso\n",
      "Score: 96\n",
      "----------------------------------------\n",
      "Roaster: El Gran Cafe\n",
      "Title: Espresso No. 6 Finca San José Ocaña\n",
      "Score: 96\n",
      "----------------------------------------\n",
      "Roaster: El Gran Cafe\n",
      "Title: Guatemala Finca Santa Isabel SL28 COE #13\n",
      "Score: 93\n",
      "----------------------------------------\n",
      "Roaster: Hula Daddy Kona Coffee\n",
      "Title: Kahiko Orange\n",
      "Score: 98\n",
      "----------------------------------------\n",
      "Roaster: Kakalove Cafe\n",
      "Title: Kenya Washed Yara Estate PB TOP\n",
      "Score: 97\n",
      "----------------------------------------\n",
      "Roaster: Hula Daddy Kona Coffee\n",
      "Title: Kona Orange\n",
      "Score: 96\n",
      "----------------------------------------\n",
      "Roaster: Utopian Coffee\n",
      "Title: Colombia Wilton Benitez Sudan Rume\n",
      "Score: 96\n",
      "----------------------------------------\n",
      "Roaster: Kakalove Cafe\n",
      "Title: Ethiopia Washed Gute 74110 Lot JH\n",
      "Score: 95\n",
      "----------------------------------------\n",
      "Roaster: Kakalove Cafe\n",
      "Title: Ethiopia Natural Sidama Bura Karamo 74158\n",
      "Score: 95\n",
      "----------------------------------------\n",
      "Roaster: Kakalove Cafe\n",
      "Title: Taiwan Honey Alishan Ming Yang Yuan Gesha\n",
      "Score: 95\n",
      "----------------------------------------\n",
      "Roaster: Kakalove Cafe\n",
      "Title: Kenya Washed Kiambu AB TOP Lot Yerihar\n",
      "Score: 94\n",
      "----------------------------------------\n",
      "Roaster: Roadmap CoffeeWorks\n",
      "Title: Kenya Ibonia Estate\n",
      "Score: 95\n",
      "----------------------------------------\n",
      "Roaster: Roadmap CoffeeWorks\n",
      "Title: Ethiopia Worka Sakaro\n",
      "Score: 94\n",
      "----------------------------------------\n",
      "Roaster: Kakalove Cafe\n",
      "Title: Guatemala Washed Huehuetenango Joya Verde\n",
      "Score: 94\n",
      "----------------------------------------\n",
      "Roaster: Kakalove Cafe\n",
      "Title: Ethiopia Washed Guji Uraga Tebe Burka\n",
      "Score: 94\n",
      "----------------------------------------\n",
      "Roaster: Kakalove Cafe\n",
      "Title: Colombia Decaf Granja Paraiso 92\n",
      "Score: 93\n",
      "----------------------------------------\n",
      "Roaster: Kakalove Cafe\n",
      "Title: Ethiopia Natural Sidama ALO Village SKY Project FL\n",
      "Score: 94\n",
      "----------------------------------------\n",
      "Roaster: Kakalove Cafe\n",
      "Title: Taiwan Natural Alishan Chi Tsai Liu Li Ecological Farm\n",
      "Score: 94\n",
      "----------------------------------------\n",
      "Roaster: JBC Coffee Roasters\n",
      "Title: Wilton Benitez SL28\n",
      "Score: 96\n",
      "----------------------------------------\n",
      "Roaster: JBC Coffee Roasters\n",
      "Title: Kolla Bolcha #6 Ethiopia\n",
      "Score: 94\n",
      "----------------------------------------\n",
      "Roaster: JBC Coffee Roasters\n",
      "Title: Bener Meriah Sumatra\n",
      "Score: 94\n",
      "----------------------------------------\n",
      "Roaster: Cozy House Coffee\n",
      "Title: Ethiopia Yirgacheffe Hama Honey\n",
      "Score: 95\n",
      "----------------------------------------\n",
      "Roaster: Cozy House Coffee\n",
      "Title: Colombia Finca Santuario Red Bourbon Natural\n",
      "Score: 95\n",
      "----------------------------------------\n",
      "Roaster: Cozy House Coffee\n",
      "Title: Ethiopia Yirgacheffe Gutiti Gargari Natural\n",
      "Score: 94\n",
      "----------------------------------------\n",
      "Roaster: Jaunt Coffee Roasters\n",
      "Title: 130 Blend\n",
      "Score: 94\n",
      "----------------------------------------\n",
      "Roaster: Coffeeland\n",
      "Title: Fruits Melody\n",
      "Score: 94\n",
      "----------------------------------------\n",
      "Roaster: Jaunt Coffee Roasters\n",
      "Title: Matriarch\n",
      "Score: 93\n",
      "----------------------------------------\n",
      "Roaster: Jaunt Coffee Roasters\n",
      "Title: Murphinator\n",
      "Score: 92\n",
      "----------------------------------------\n",
      "Roaster: Coffeeland\n",
      "Title: Taiwan Symphony\n",
      "Score: 92\n",
      "----------------------------------------\n",
      "Roaster: Coffeeland\n",
      "Title: Blue Notes\n",
      "Score: 91\n",
      "----------------------------------------\n",
      "Roaster: Speedwell Coffee\n",
      "Title: Ethiopia Dale\n",
      "Score: 93\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# To get the detailed review data from each coffee review url\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "coffee_review_data = []\n",
    "\n",
    "\n",
    "def create_session_with_retries():\n",
    "    \"\"\"\n",
    "    Creates a requests session with automatic retry mechanism, it will retry maximum 5 times\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    session = requests.Session()\n",
    "    \n",
    "    retries = Retry(\n",
    "        total=5,  # total times of retries\n",
    "        backoff_factor=1,  # wait  (2 ** retry times)--> 1s, 2s, 4s, 8s, 16s between retries\n",
    "        status_forcelist=[500, 502, 503, 504],  # HTTP status codes that need to be retried\n",
    "        allowed_methods=[\"GET\"]\n",
    "    )\n",
    "    \n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    \n",
    "    return session\n",
    "\n",
    "\n",
    "\n",
    "def scrape_detailed_review(review_url):\n",
    "    \"\"\"suScrape detailed coffee review information from a given URL\n",
    "    \n",
    "    Args:\n",
    "        review_url (str): The URL of the coffee review page\n",
    "    Returns:\n",
    "        None\n",
    "    Raises:\n",
    "        Exception: If there is an error during the request or processing\n",
    "    \"\"\"\n",
    "    \n",
    "    session = create_session_with_retries()\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    #response = requests.get(review_url, headers=headers)  #--> it is not working, timeout error\n",
    "\n",
    "    try:\n",
    "        #response = session.get(review_url, headers=headers, timeout=(10, 60))  \n",
    "        # ConnectionError: ('Connection aborted.', TimeoutError(60, 'Operation timed out'))\" \n",
    "\n",
    "        # Change to use session to send request\n",
    "        response = session.get(\n",
    "            review_url, \n",
    "            headers=headers, \n",
    "            timeout=(30, 60)  #if connection >30s or read >60s, it will raise error and retry\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            try:\n",
    "                time.sleep(random.uniform(1, 2))\n",
    "\n",
    "                # -First: Find the title and roaster company\n",
    "                # Find the roaster company\n",
    "                roaster_element = soup.find('p', class_='review-roaster')\n",
    "                if roaster_element:\n",
    "                    roaster = roaster_element.text.strip()\n",
    "                else:\n",
    "                    roaster = 'No data'\n",
    "\n",
    "                # to find h1 /h2 tag , and the class 'review-title' or 'entry-title'\n",
    "                title_element = soup.find(['h1', 'h2'], class_=['review-title', 'entry-title'])\n",
    "                \n",
    "                # if the title element is found, extract the text and remove whitespace; if not, use 'No Title'\n",
    "\n",
    "                if title_element:\n",
    "                    title = title_element.text.strip()\n",
    "                else:\n",
    "                    title = 'No Title'\n",
    "                    \n",
    "                # -Second: Find the score\n",
    "                # Find the score in the webpage\n",
    "                score_element = soup.find('span', class_='review-template-rating')\n",
    "                # If the score element is found, extract the text and remove whitespace; if not, use 'No Score'\n",
    "                \n",
    "                if score_element:\n",
    "                    score = score_element.text.strip()\n",
    "                else:\n",
    "                    score = 'No data'\n",
    "\n",
    "                # -Third: Find the detailed information table\n",
    "                # Find all tables in the webpage\n",
    "                \n",
    "                tables = soup.find_all('table')  # Find all tables in the webpage\n",
    "\n",
    "                attributes = {} # initialize an empty dictionary\n",
    "\n",
    "\n",
    "                # go through all tables, and to find the table that has two columns\n",
    "                for table in tables:\n",
    "                    rows = table.find_all('tr')\n",
    "                    for row in rows:\n",
    "                        cells = row.find_all('td')\n",
    "                        if len(cells) == 2:\n",
    "                            key = cells[0].text.strip().rstrip(':')  \n",
    "                            value = cells[1].text.strip()\n",
    "                            attributes[key] = value\n",
    "                \n",
    "\n",
    "                # Print  Table Data\n",
    "                print(f\"Roaster: {roaster}\")\n",
    "                print(f\"Title: {title}\")\n",
    "                print(f\"Score: {score}\")\n",
    "                \n",
    "                #print(\"Other Attributes:\")\n",
    "                #for key, value in attributes.items():\n",
    "\n",
    "                #    print(f\"  {key}: {value}\")\n",
    "                    \n",
    "            \n",
    "                print(\"-\" * 40)\n",
    "\n",
    "                #get all data in a list\n",
    "                coffee_review_data.append({\n",
    "                    'title': title,\n",
    "                    'roaster': roaster,\n",
    "                    'score': score,\n",
    "                    **attributes # attributes dictionary unpacking\n",
    "                \n",
    "                })\n",
    "            except AttributeError as e:\n",
    "    \n",
    "                print(f\"Failed to get detail data from {review_url}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Request Error: {review_url}: {e}\")\n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the coffee review data scraping process\n",
    "    \n",
    "    Args:\n",
    "        None\n",
    "        \n",
    "    Returns:\n",
    "        None: Saves scraped data to 'coffee_review_data.csv'\n",
    "        \n",
    "    Process:\n",
    "        1. Retrieves coffee review URLs from df_coffee_brand\n",
    "        2. Iterates through each URL to scrape review details\n",
    "        3. Converts collected data to DataFrame\n",
    "        4. Saves data to CSV file after each successful scrape\n",
    "        5. Implements random delays between requests\n",
    "    \"\"\"\n",
    "\n",
    "    review_links = df_coffee_brand['coffee_review_url']\n",
    "    \n",
    "    \n",
    "    for link in review_links:  # for-loop to go through each link\n",
    "        scrape_detailed_review(link)\n",
    "         \n",
    "        df_coffee_review_data = pd.DataFrame(coffee_review_data)\n",
    "        df_coffee_review_data.to_csv('coffee_review_data.csv', index=False)\n",
    "        \n",
    "        time.sleep(random.uniform(5, 10))  # randomly delay between requests\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### After scraping, we already got the coffee review data. Secondly, We need to merge the coffee brand data with the coffee review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       title                          roaster  \\\n",
      "0             Kenya AA Aries Return Espresso  Simon Hsieh Aroma Roast Coffees   \n",
      "1         Kenya All’s Well Peaberry Espresso  Simon Hsieh Aroma Roast Coffees   \n",
      "2        Espresso No. 6 Finca San José Ocaña                     El Gran Cafe   \n",
      "3  Guatemala Finca Santa Isabel SL28 COE #13                     El Gran Cafe   \n",
      "4                              Kahiko Orange           Hula Daddy Kona Coffee   \n",
      "\n",
      "   score              Roaster Location  \\\n",
      "0     96               Taoyuan, Taiwan   \n",
      "1     96               Taoyuan, Taiwan   \n",
      "2     96  Antigua Guatemala, Guatemala   \n",
      "3     93  Antigua Guatemala, Guatemala   \n",
      "4     98             Holualoa, Hawai’i   \n",
      "\n",
      "                                       Coffee Origin   Roast Level Agtron  \\\n",
      "0          Nyeri growing region, south-central Kenya        Medium  44/60   \n",
      "1          Nyeri growing region, south-central Kenya        Medium  46/72   \n",
      "2  San Juan Sacatepéquez, Antigua Department, Gua...        Medium  50/64   \n",
      "3  San Cristobal Verapaz, Alta Verapaz, Antigua D...        Medium  52/60   \n",
      "4  Holualoa, North Kona growing district, “Big Is...  Medium-Light  54/84   \n",
      "\n",
      "         Est. Price    Review Date  Aroma  Body  Flavor  Aftertaste  \\\n",
      "0  NT $950/8 ounces  November 2024      9     9       9           9   \n",
      "1  NT $900/8 ounces  November 2024      9     9      10           9   \n",
      "2  $25.00/12 ounces  November 2024      9     9       9           9   \n",
      "3  $47.00/12 ounces  November 2024      9     8       9           8   \n",
      "4   $99.95/8 ounces  November 2024     10     9      10           9   \n",
      "\n",
      "   With Milk  Acidity/Structure                                coffee_name  \\\n",
      "0       10.0                NaN             Kenya AA Aries Return Espresso   \n",
      "1        9.0                NaN         Kenya All’s Well Peaberry Espresso   \n",
      "2       10.0                NaN        Espresso No. 6 Finca San José Ocaña   \n",
      "3        NaN                9.0  Guatemala Finca Santa Isabel SL28 COE #13   \n",
      "4        NaN               10.0                              Kahiko Orange   \n",
      "\n",
      "                                   coffee_review_url  \\\n",
      "0  https://www.coffeereview.com/review/kenya-aa-a...   \n",
      "1  https://www.coffeereview.com/review/kenya-alls...   \n",
      "2  https://www.coffeereview.com/review/espresso-n...   \n",
      "3  https://www.coffeereview.com/review/guatemala-...   \n",
      "4  https://www.coffeereview.com/review/kahiko-ora...   \n",
      "\n",
      "                                  coffee_brand_url  \n",
      "0  https://www.facebook.com/4ArtsZeroDefectCoffees  \n",
      "1  https://www.facebook.com/4ArtsZeroDefectCoffees  \n",
      "2                             https://elgran.cafe/  \n",
      "3                             https://elgran.cafe/  \n",
      "4                            http://bit.ly/2oBDGmP  \n"
     ]
    }
   ],
   "source": [
    "# read the coffee review data\n",
    "df_coffee_review_data = pd.read_csv('coffee_review_data.csv')\n",
    "\n",
    "# read the coffee brand data\n",
    "df_coffee_brand = pd.read_csv('coffee_brand.csv')\n",
    "\n",
    "# merge two dataframe\n",
    "df_merged = pd.merge(\n",
    "    df_coffee_review_data, \n",
    "    df_coffee_brand[['coffee_name', 'coffee_review_url', 'coffee_brand_url']], \n",
    "    left_on='title',  # the column in df_coffee_review_data\n",
    "    right_on='coffee_name',  # the column in df_coffee_brand\n",
    "    how='left'\n",
    ")\n",
    "# print the merged dataframe\n",
    "print(df_merged.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### In new dataframe, we had already merged two dataframe. Then, We have to find the Geographic Location of the `Coffee Origin`.\n",
    "#### To check the `Coffee Origin` is in which country, if it is not in the country, we need to find the country of the `Coffee Origin`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Coffee Origin\n",
       "Piendamo, Cauca Department, Colombia                                  3\n",
       "Sidamo growing region, southern Ethiopia                              3\n",
       "Nyeri growing region, south-central Kenya                             2\n",
       "Holualoa, North Kona growing district, “Big Island” of Hawai’i        2\n",
       "Alishan, Taiwan                                                       2\n",
       "Yirgacheffe growing region, south-central Ethiopia                    2\n",
       "Aceh, Sumatra, Indonesia                                              1\n",
       "Guatemala; Colombia; Brazil; Myanmar                                  1\n",
       "Taiwan                                                                1\n",
       "Mexico, Papua New Guinea                                              1\n",
       "Brazil; Guatemala; Ethiopia                                           1\n",
       "Ethiopia                                                              1\n",
       "Guatemala; Ethiopia                                                   1\n",
       "Cauca Department, Colombia                                            1\n",
       "Guji Zone, Oromia Region, southern Ethiopia                           1\n",
       "Agaro, Oromia region, Ethiopia                                        1\n",
       "San Juan Sacatepéquez, Antigua Department, Guatemala                  1\n",
       "Huehuetenango, Guatemala                                              1\n",
       "Gedeb District, Gedeo Zone, southern Ethiopia                         1\n",
       "Kiambu County, Kenya                                                  1\n",
       "Kiambu County, south-central Kenya                                    1\n",
       "Ruiru, Kiambu County, south-central Kenya                             1\n",
       "San Cristobal Verapaz, Alta Verapaz, Antigua Department, Guatemala    1\n",
       "Wensho Woreda, Sidamo growing region, Ethiopia                        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the unique value of 'Coffee Origin'\n",
    "df_merged['Coffee Origin'].unique()\n",
    "df_merged['Coffee Origin'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the result, \n",
    "# we could see that there are some coffee from few locations. \n",
    "# Aslo, there are no logical rules to classify the coffee origin, \n",
    "# so try to use fuzzy matching and regular expression to match the region.\n",
    "\n",
    "# For example: \n",
    "# 1) Brazil; Guatemala; Ethiopia  (country ; country ; country)\n",
    "# 2) Mexico, Papua New Guinea   (country , country)\n",
    "# 3) Kiambu County, south-central Kenya        (county name, country)\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Identify single-origin coffees\n",
    "# Add a new column [is_blend]in df_merged to indicate whether it's single-origin or blend (0:single-origin, 1:blend)\n",
    "\n",
    "# Step 2: Region Classification Logic:\n",
    "# Create binary indicators (0 or 1) for these columns:\n",
    "# - region_africa_arabia\n",
    "# - region_caribbean\n",
    "# - region_central_america\n",
    "# - region_hawaii\n",
    "# - region_asia_pacific\n",
    "\n",
    "#Step 3: To classify the coffee origin\n",
    "# For single-origin coffees (separated by ','): Direct classification\n",
    "# For blended coffees (separated by ';'): \n",
    "\n",
    "# - First identify all origin regions (2 or 3)\n",
    "# - Check each region against the 5 geographical areas\n",
    "# - Mark '1' for matching regions\n",
    "\n",
    "# Step 4: Display results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\Result:\n",
      "                                       Coffee Origin  is_blend  \\\n",
      "0          Nyeri growing region, south-central Kenya         0   \n",
      "1          Nyeri growing region, south-central Kenya         0   \n",
      "2  San Juan Sacatepéquez, Antigua Department, Gua...         0   \n",
      "3  San Cristobal Verapaz, Alta Verapaz, Antigua D...         0   \n",
      "4  Holualoa, North Kona growing district, “Big Is...         0   \n",
      "5          Ruiru, Kiambu County, south-central Kenya         0   \n",
      "6  Holualoa, North Kona growing district, “Big Is...         0   \n",
      "7               Piendamo, Cauca Department, Colombia         0   \n",
      "8           Sidamo growing region, southern Ethiopia         0   \n",
      "9           Sidamo growing region, southern Ethiopia         0   \n",
      "\n",
      "   region_africa_arabia  region_caribbean  region_central_america  \\\n",
      "0                     1                 0                       0   \n",
      "1                     1                 0                       0   \n",
      "2                     0                 0                       1   \n",
      "3                     0                 0                       1   \n",
      "4                     0                 0                       0   \n",
      "5                     1                 0                       0   \n",
      "6                     0                 0                       0   \n",
      "7                     0                 0                       0   \n",
      "8                     1                 0                       0   \n",
      "9                     1                 0                       0   \n",
      "\n",
      "   region_hawaii  region_asia_pacific  \n",
      "0              0                    0  \n",
      "1              0                    0  \n",
      "2              0                    0  \n",
      "3              0                    0  \n",
      "4              1                    0  \n",
      "5              0                    0  \n",
      "6              1                    0  \n",
      "7              0                    0  \n",
      "8              0                    0  \n",
      "9              0                    0  \n"
     ]
    }
   ],
   "source": [
    "# import regular expressions package\n",
    "import re\n",
    "\n",
    "def create_region_patterns():\n",
    "    \"\"\"Create region matching patterns\n",
    "\n",
    "    Args:\n",
    "        None\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing region patterns and associated keywords   \n",
    "    \n",
    "    Process:\n",
    "        if the country is in the 'countries' list, return True\n",
    "        otherwise, use the keywords to check if the country matches the pattern (fuzzy matching and regular expression)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    regions = {\n",
    "        'africa_arabia': {\n",
    "            'countries': ['Ethiopia', 'Kenya', 'Tanzania', 'Rwanda', 'Uganda', 'Yemen', 'Burundi', 'Congo'],\n",
    "            'keywords': ['ethiopia', 'ethiopian', 'yirgacheffe', 'sidamo', 'guji',\n",
    "                         'kenya', 'kenyan', 'nyeri', 'kiambu',\n",
    "                         'tanzania', 'tanzanian',\n",
    "                         'rwanda', 'rwandan',\n",
    "                         'uganda', 'ugandan',\n",
    "                         'yemen', 'yemeni',\n",
    "                         'burundi', 'burundian',\n",
    "                         'congo', 'congolese']\n",
    "        },\n",
    "        'caribbean': {\n",
    "            'countries': ['Jamaica', 'Haiti', 'Dominican Republic', 'Cuba'],\n",
    "            'keywords': ['jamaica', 'jamaican', 'blue mountain',\n",
    "                         'haiti', 'haitian',\n",
    "                         'dominican', 'dominica',\n",
    "                         'cuba', 'cuban']\n",
    "        },\n",
    "        'central_america': {\n",
    "            'countries': ['Guatemala', 'Costa Rica', 'Honduras', 'El Salvador', 'Nicaragua', 'Panama', 'Mexico'],\n",
    "            'keywords': ['guatemala', 'guatemalan', 'antigua', 'huehuetenango', 'atitlan',\n",
    "                         'costa rica', 'costa rican', 'tarrazu',\n",
    "                         'honduras', 'honduran',\n",
    "                         'el salvador', 'salvadoran',\n",
    "                         'nicaragua', 'nicaraguan',\n",
    "                         'panama', 'panamanian',\n",
    "                         'mexico', 'mexican', 'chiapas', 'oaxaca']\n",
    "        },\n",
    "        'hawaii': {\n",
    "            'countries': ['Hawaii', 'Kona', 'Maui', 'Hawai'],\n",
    "            'keywords': ['hawaii', 'hawaiian', 'kona', 'maui', 'big island']\n",
    "        },\n",
    "        'asia_pacific': {\n",
    "            'countries': ['Indonesia', 'Papua New Guinea', 'Taiwan', 'Vietnam', 'Thailand', 'India', 'Myanmar'],\n",
    "            'keywords': ['indonesia', 'indonesian', 'sumatra', 'sulawesi', 'java',\n",
    "                         'papua', 'new guinea',\n",
    "                         'taiwan', 'taiwanese', 'alishan',\n",
    "                         'vietnam', 'vietnamese',\n",
    "                         'thailand', 'thai',\n",
    "                         'india', 'indian', 'malabar',\n",
    "                         'myanmar', 'burmese']\n",
    "        }\n",
    "    }\n",
    "    # 將關鍵詞列表轉為正則表達式\n",
    "    for region, info in regions.items():\n",
    "        info['patterns'] = [r'|'.join(info['keywords'])]\n",
    "    return regions\n",
    "\n",
    "\n",
    "def is_in_region(place, region_info):\n",
    "    \"\"\"檢查地點是否屬於特定地區\"\"\"\n",
    "    if pd.isna(place):\n",
    "        return False\n",
    "        \n",
    "    place = str(place).lower()\n",
    "    \n",
    "    # 2 Ways to mapping:\n",
    "    # a) Direct matching -> check the exact country name\n",
    "    if any(country.lower() in place for country in region_info['countries']):\n",
    "        return True\n",
    "    \n",
    "    # b) Fuzzy matching -> use the pattern to check if the country matches the pattern\n",
    "    return any(re.search(pattern, place, re.IGNORECASE) \n",
    "              for pattern in region_info['patterns'])\n",
    "\n",
    "\n",
    "def classify_region(origin):\n",
    "    \"\"\"Classify coffee origins into specific geographical regions\n",
    "    \n",
    "    Args:\n",
    "        origin (str): Coffee origin string, can be single origin or blend\n",
    "                     Format examples: \n",
    "                     - Single origin: \"Ethiopia, Yirgacheffe\"\n",
    "                     - Blend: \"Ethiopia; Colombia\"\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing binary indicators for each region:\n",
    "            {\n",
    "                'region_africa_arabia': 0 or 1,\n",
    "                'region_caribbean': 0 or 1,\n",
    "                'region_central_america': 0 or 1,\n",
    "                'region_hawaii': 0 or 1,\n",
    "                'region_asia_pacific': 0 or 1,\n",
    "                'is_blend': 0 or 1  # 1 if origin contains ';'\n",
    "            }\n",
    "    \"\"\"\n",
    "    \n",
    "    if pd.isna(origin):\n",
    "        return {\n",
    "            'region_africa_arabia': 0,\n",
    "            'region_caribbean': 0,\n",
    "            'region_central_america': 0,\n",
    "            'region_hawaii': 0,\n",
    "            'region_asia_pacific': 0,\n",
    "            'is_blend': 0\n",
    "        }\n",
    "    \n",
    "    # Use the function to get the region matching \n",
    "    regions = create_region_patterns()\n",
    "    \n",
    "    # To check if the origin is blend \n",
    "    is_blend = 1 if ';' in str(origin) else 0\n",
    "    \n",
    "    # Use ';' or ',' to split the origin\n",
    "    if is_blend:\n",
    "        origins = [o.strip() for o in str(origin).split(';')]\n",
    "    else:\n",
    "        origins = [o.strip() for o in str(origin).split(',')]\n",
    "    \n",
    "    # Initialize \n",
    "    result = {\n",
    "        'region_africa_arabia': 0,\n",
    "        'region_caribbean': 0,\n",
    "        'region_central_america': 0,\n",
    "        'region_hawaii': 0,\n",
    "        'region_asia_pacific': 0,\n",
    "        'is_blend': is_blend\n",
    "    }\n",
    "    \n",
    "    # To check each origin in the list\n",
    "    for place in origins:\n",
    "        if is_in_region(place, regions['africa_arabia']):\n",
    "            result['region_africa_arabia'] = 1\n",
    "        if is_in_region(place, regions['caribbean']):\n",
    "            result['region_caribbean'] = 1\n",
    "        if is_in_region(place, regions['central_america']):\n",
    "            result['region_central_america'] = 1\n",
    "        if is_in_region(place, regions['hawaii']):\n",
    "            result['region_hawaii'] = 1\n",
    "        if is_in_region(place, regions['asia_pacific']):\n",
    "            result['region_asia_pacific'] = 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Apply the classify_region to DataFrame\n",
    "results = df_merged['Coffee Origin'].apply(classify_region)\n",
    "\n",
    "# Add the results to DataFrame\n",
    "for key in ['region_africa_arabia', 'region_caribbean', 'region_central_america', \n",
    "           'region_hawaii', 'region_asia_pacific', 'is_blend']:\n",
    "    df_merged[key] = results.apply(lambda x: x[key])\n",
    "\n",
    "# Print the results\n",
    "print(f\"\\Result:\")\n",
    "print(df_merged[['Coffee Origin', 'is_blend', 'region_africa_arabia', \n",
    "                'region_caribbean', 'region_central_america', \n",
    "                'region_hawaii', 'region_asia_pacific']].head(10))\n",
    "\n",
    "# Save the results to csv\n",
    "df_merged.to_csv('coffee_data_with_regions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'NT $950'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoffee_data_with_regions.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# convert the price to USD\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEst. Price USD\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEst. Price\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_price_to_usd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# print the results\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCovert Result:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Springboard/lib/python3.8/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Springboard/lib/python3.8/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Springboard/lib/python3.8/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/Springboard/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[12], line 28\u001b[0m, in \u001b[0;36mconvert_price_to_usd\u001b[0;34m(price_str)\u001b[0m\n\u001b[1;32m     25\u001b[0m     unit_part \u001b[38;5;241m=\u001b[39m price_str\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# convert the price to float\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     amount \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprice_part\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mamount\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m0.03\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit_part\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# if the price is in USD, return the price\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'NT $950'"
     ]
    }
   ],
   "source": [
    "# Convert the price to USD\n",
    "def convert_price_to_usd(price_str):\n",
    "    \"\"\"Convert price string to USD format\n",
    "    \n",
    "    Args:\n",
    "        price_str (str): Price string that may contain NT$ or $ with amount\n",
    "        \n",
    "    Returns:\n",
    "        str: Converted price in USD format (e.g., \"$15.99/12 oz\")\n",
    "        None: If input is NaN or invalid\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if pd.isna(price_str): # if the price is NaN, return None\n",
    "        return None\n",
    "        \n",
    "    #  Convert to string and remove whitespace\n",
    "    price_str = str(price_str).strip()\n",
    "    \n",
    "    # to check if the price is in NT$\n",
    "    if 'NT' in price_str:\n",
    "        # split the price and unit\n",
    "        price_part = price_str.replace('NT$', '').split('/')[0]\n",
    "        unit_part = price_str.split('/')[-1]\n",
    "        \n",
    "        # convert the price to float\n",
    "        amount = float(price_part.replace(',', ''))\n",
    "        return f\"${amount * 0.03:.1f}/{unit_part}\"\n",
    "    else:\n",
    "        # if the price is in USD, return the price\n",
    "        return price_str\n",
    "\n",
    "# read the data\n",
    "data = pd.read_csv('coffee_data_with_regions.csv')\n",
    "\n",
    "# convert the price to USD\n",
    "data['Est. Price USD'] = data['Est. Price'].apply(convert_price_to_usd)\n",
    "\n",
    "# print the results\n",
    "print(f\"\\Covert Result:\")\n",
    "print(data[['Est. Price', 'Est. Price USD']].head(10))\n",
    "\n",
    "# save to csv\n",
    "data.to_csv('coffee_data_with_usd_prices.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the data that has null value in 'Score'\n",
    "data = data[data['score'].notna()]\n",
    "\n",
    "\n",
    "# Drop the data that has null value in [Aroma, Body, Flavor, Aftertaste]\n",
    "data = data[data['Aroma'].notna()]\n",
    "data = data[data['Body'].notna()]\n",
    "data = data[data['Flavor'].notna()]\n",
    "data = data[data['Aftertaste'].notna()]\n",
    "\n",
    "\n",
    "# save the data to csv\n",
    "data.to_csv('coffee_data_cleaned.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert 'Roast location' to Geolocation (latitude and longitude)\n",
    "df_cleaned = pd.read_csv('coffee_data_cleaned.csv')\n",
    "\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "\n",
    "for index, row in df_cleaned.iterrows():\n",
    "    try:\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "        \n",
    "        location = geolocator.geocode(row['Roaster Location'])\n",
    "        df_cleaned.at[index, 'Latitude'] = location.latitude\n",
    "        df_cleaned.at[index, 'Longitude'] = location.longitude  \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "print(df_cleaned.head())\n",
    "\n",
    "# save the data to csv\n",
    "df_cleaned.to_csv('coffee_data_with_location.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the location, to get the weather data\n",
    "# Not able to get the weather data based on the geolocation data yet\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewKernel",
   "language": "python",
   "name": "my_new_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
